{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLaP26xnQKrU56vfev3qyy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["class Agent:\n","  def __init__(self):\n","    self.state = None\n","    self.action = None\n","    self.reward = None\n","\n","  def take_action(self, environment):\n","    self.action = self.policy(self.state)\n","    environment.step(self.action)\n","    self.state = environment.state\n","    self.reward = environment.reward\n","\n","  def policy(self, state):\n","    # TODO: Implement policy function.\n","    return None\n","\n","class Environment:\n","  def __init__(self):\n","    self.state = None\n","    self.reward = None\n","\n","  def step(self, action):\n","    # TODO: Implement step function.\n","    pass\n","\n","  def reset(self):\n","    # TODO: Implement reset function.\n","    pass\n","\n","def train(agent, environment, num_episodes):\n","  for episode in range(num_episodes):\n","    agent.reset()\n","    environment.reset()\n","\n","    while not environment.is_done():\n","      agent.take_action(environment)\n","\n","    print(\"Episode {}: Reward = {}\".format(episode, agent.reward))\n","\n","if __name__ == \"__main__\":\n","  agent = Agent()\n","  environment = Environment()\n","\n","  train(agent, environment, 100)"],"metadata":{"id":"iHShQnutIia3","colab":{"base_uri":"https://localhost:8080/","height":331},"executionInfo":{"status":"error","timestamp":1702355533837,"user_tz":-330,"elapsed":416,"user":{"displayName":"navin shrivatsan","userId":"02096111809868742973"}},"outputId":"eaf18645-e2cd-4052-aef2-4c7739821601"},"execution_count":2,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9b0e68a3799f>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-9b0e68a3799f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, environment, num_episodes)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'reset'"]}]}]}